---
title: "marlin-paper-notes"
author: "Dan Ovando"
date: "4/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

what dooes the overlap tradeoff curve look like? 

Suppose you close off 30% of an area in an MPA

Then plot conservation outcome on y-axis, degree of overlap between target and bycatch species on the x axis. When overlap is one, conservation benefit should be maxed. Then overlap is perfeclty anticorrelated, if effort concentrates, conservation should be at some negative minimum. If concentration doesn't happen, then conservation effect should be zero. If overlap is uncorrelated... unclear?

MPA literature has been dominated by single-species model covering homogemous habitat without any technical interactions across multiple fleets and species. 

@hastings2017 assumes that adults are stationary, larvae are perfectly mixed, age structure can be ignored, density dependence at time of settlement. Same harvest rate outside reserve. 

static spatial, dynamic spatial , input / output controls, gear

If there is perfect overlap, then improvement in coservation outcome should be a straight positive function of area protected. If there is no overlap, then 


# what's the angle?

Smith et al and references therein make the basic case pretty damn clear: when species move a lot and when habitats overlap, static MPAs don't work as well as dynamic. There are other nuances in there but that kind of summarizes a lot of it

so what are the specific angles this paper can take

- When do static closures go from "less effective" to "not effective"

Those papers point out "less effective" since max species = 2 to 4. But, we're seeing here that non-trivial case of net loss? But is it really non-trivial? like 15% of simulations with more than 10%? though that number is a lot bigger if you're basically asking "did it help"

So that's certainly an angle that's unexplored here. 


- Yet another comparison to active fisheries management

Tying more directly to fisheries management, compare to say a TAE, in terms of units bycatch saved per unit of target catch lost 

Could also do the dynamic comparison, though that one is a bit boring to me since it's sort of self evident, so question is the wedge?

Could also ask something about how much would gear efficiency have to change, though it feels like that would be so detail derived it would be hard to make general

- Value of information




- A more tactical evaluation

Lean in a bit more to an area and specific existing or proposed MPAs with best estimates of bycatch risk / target fleets in space. 

The cleanest to what we have now, but also the most contentious / work. If we go with actual MPAs, really need to dial in the species and fleets, which we're pretty close to do be honest, but also the habitats etc. 

Though there is a more general question like "what's the best outcome you could get for bycatch with static closures in a place that looks like the WCPO". Or rather what design gets you the best outcome. Now you'd have to put a constraint on that, say total catches can't decline by more than xx%?

Hmm I like this one sort of

- incentives piece around bycatch
incentive to fish and incentive to protect

So this isn't the odd rare turtle, these are valuable sharks


## optimization

similar SIR approach... why not just get it running first

Yields: sum of revenue of target species: or if you just say revenue, then you can manipulate prices to get desired impact: if you say that bycatch literally has no value then just set price to 0, or negative. 

biodiversity: B/K of all species (for now): average or sum? Sum I think, since you'd rather have all at 0.5 than half at 0 and half at 1

yield_weight: do you have MSY for all species? I don't think so, and gets crazy complicated to think about MSY once you start messing with technical interactions etc. So you just need something to weight the marginal value of both. 

So, for every step, you'll have N candidate cells, each with a marginal yield effect and a marginal biodiversity effect. You then need to rank each cell by the objective function. Need some function to convert yield and biomass into comparable units then. So just center and scale? alpha then controls the relative weight of a standard deviation increase in yield relative to a standard deviation increase in biodiversity? Issue there: once you're down to the last cell this won't work. Dividing by max caps both at 1, but no limit to the loss there... Rescale to be positive? yield - min(yield) + 1e-3 will ensure the min value is 1e-3, then scaling by max of that vector will make the max 1, so now everything has the same range. Oh, or just use scaes::rescale to put both on range of 0 to 1. 

Is that going to have weird behavior rescaling at every iteration like that? i.e. the effective value of alpha will change depending on the scale of the biomass and yield. Setting alpha to 1 will maximize yields, setting it to 0 will maximize biodiversity, what does setting to 0.5 mean? value the best yield cell as much as the best biomass cell. 

Seems good enough for now. So doing it this way, you only have to iterate over the weight, no different objective function shapes

So what are you optimizing? Location and size or just location? In theory, you can save some computation time by stopping the optimization when the objective function starts to consistently decline. But, since there might be multiplie maxima here, that could get tricky, and wouldn't allow you to look at tradeoff wedges. So...

- for each weight
- for each range of MPA sizes
- find the optimal MPA configuration. 
- Not too bad!

## what are you comparing optimization to

OK, you have some optimized results, so what. On the one hand, you can examine to what extent given this case study, tradeoffs across species or across yields are inevitable or just gotta find the right network. 

From there though, can also compare "optimal" to "realistic"

Design strategies
1. Assign protection by SSB / SSB0 weighted abundance in space (protect the highest concentrations of the most threatened species)
2. Assign protection by bycatch rates (distribution of biomass bycatch / biomass target)
3. Assign protection by targeting fishing effort
4. Assign protection by avoiding fishing effort

Knowledge states
1. Degrees of accuracy over the thing being measured (i.e. error in the design strategy)

2. Number of species considered (e.g. design around only 2 bycatch species instead of all)

3. Ã¼ber naive: just close off random patches up to a specified size

We then plot these points in objective function space and compare to the efficiency frontier / consider frequency and magnitude of conservation and economic tradeoofs

The question all this is answering: how sensitive are conservation and economic outcomes to design strategy / uncertainty when there are technical interactions and heterogeneity in habitat? The first stage is teeing up the idea that beyond conservation / yield tradeoffs, as you increase habitat heterogeneity with technical interactions you increase the potential for conservation tradeoffs. But, the degree is going to be super context dependent, so to dive in a bit we then focus on this case study of the particular species / habitat overlap in the WCPO. Further work might reveal more generalities in this, but htis is basically getting a sense for the potential magnitude of the challenge. 


OK, this is actually coming together! You should run the same design / knowledge states for the broader range of simulated habitats. That will make the point nice and clearly. 

But, that begs the question of, is it an inevitable tradeoff, or just a case of poor design. That requires optimization. But, optimization is very computationally intensive. To examine this point then, we repeated the exercies, but with a specific case study where we hold habitat as constant, but redo the exercise conditional on constant habitat. We can then see to what extent tradeoofs persist, and ideally come up with some way of comparing the case study habitats to the metric of habitat heterogeneity in the full simulations. Case study also demonstrates that its a real thing: not making up the idea of different habitat use

This might actually be a cool paper. Proc B?

Idea: what ig instead of local or global, you have a parameter that says how many metapopulations there are. 1 says global density dependence, patches says one per patch. you then divide up the patches in equal aquares based on that, and calculate density dependence on that level. 


## 2020-06-15 check in call

Framing - MPAs and bycatch for large MPAs- what is the impact of MPAs on bycatch in the pelagic

We don't have the right tools in the toolkit - better presentation of marlin - conventional models. We've packaged this up in a neat little package called marlin 

Hone in on what are the research questions

- How big to get a big effect

- How significant of a risk are unintended consequences

- How do benefits and risk map onto economic tradeoffs 



- Potential for unintended consequences is real

- But, since has to reach 0 at 100%, hard for effects to be that negative: either MPA is small so small effect, or MPA starts to get large enough to produce some protection too. 
  - But that's clearly context dependent, wouldn't hold if you have a critter that only lives in one patch and you don't protect it

- Some design paradigms are better than others but even good designs struggle as information quality goes down

- Optimization can help, but can't remove tradeoffs

- Unintended consequences are a real issue that need to be considered any time you have effort displacement and non-perfectly overlapping species, but problem isn't severe enough given reasonable parameters as an argument for doing nothing say


- Add initial status as a thing to vary over - largely depends on their starting status 

- Build intuition over life history and 

Hi Dan! Thanks for sending the marlin paper outline. Just a few thoughts in advance of our meeting.
I think the message can be much simpler than what is currently written. The key question is simply: can large offshore MPAs effectively protect species threatened by fishing? We can bring in stats on how overfishing drives extinction risk in many threatened species (e.g. sharks). Then we can make a case for what is different about the pelagic environment from the nearshore environment, and how models and empirics currently focus on the nearshore. We can then isolate the parameters of interest in the offshore...which brings me to my second framing point,
There is a tension between 'we are just extending hastings et al. to explicitly relax assumptions made for nearshore environments to understand their implications on mpa impacts on pelagic bycatch' and 'here is a new method that is needed to explore this question, I.e. marlin).' We should discuss this and figure out which case we want to make or if there is a real compromise there. And finally,
These decisions will help us decide which specific research questions we are ultimately interested in. Currently, because we don't have clear research questions, it is almost impossible to figure out how to best position our results (which is why I think the figures are a bit all over the place). I would suggest bullet-ing out the 3-5 most exciting findings from the work, and then we can use those to inform our conversation about the framing of the paper and presentation of results.


LENNON

Walk through some of the figures and unpack them a bit


# comparing catches to CPUE to abundance

```{r}

rfmo <- "wcpfc"
mats <- list.files(path = here("data", "tuna-catch"))

# mats <- mats[str_detect(mats, "wcpfc_")]

mats <- mats[str_detect(mats, paste0(rfmo,"_")) & str_detect(mats, "tonnes") & str_detect(mats, "monthly") & str_detect(mats, "2017") & str_detect(mats, "5grid")]

get_layer <- function(file) {
  # file <- mats[7]

  file_comps <- str_split(file, "_", simplify = TRUE)
  
  species <- paste(file_comps[,7],file_comps[,8])
  
  species <- str_replace(species, ".tif", "")
# 
  hab <- stars::read_stars(here("data", "tuna-catch", file)) %>%
    sf::st_as_sf() %>%
    recenter_vector(center= -200)

#   plot(hab)
  
  centers <- sf::st_centroid(hab)
  
  centers <- sf::st_coordinates(centers) %>% 
    as.data.frame()
  
  centers$catch = as.data.frame(hab)[,1]
    

  x_binsize <- (max(centers$X) - min(centers$X) + 1) / resolution

  y_binsize <- (max(centers$Y) - min(centers$Y) + 1) / resolution

  if (x_binsize < 1 | y_binsize < 1) {
    stop("resolution is too high for input data")
  }


  habitat <- centers %>%
    mutate(X = X - min(X),
           Y = Y - min(Y)) %>%
    mutate(rough_x = floor(X / x_binsize),
           rough_y = floor(Y / y_binsize)) %>%
    dplyr::select(contains("rough_"), catch)


  habitat <- habitat %>%
    group_by(rough_x, rough_y) %>%
    summarise(hab = sum(catch)) %>%
    mutate(xy = rough_x * rough_y)

  mod <-
    gamm4::gamm4(hab ~ s(rough_x) + s(rough_y) + s(xy), data = habitat)

  habitat_frame <-
    expand_grid(rough_x = 1:resolution,
                rough_y = 1:resolution) %>%
    mutate(xy = rough_x * rough_y)
  
interp_habitat <- predict(mod$gam, newdata = habitat_frame)
  
interp_habitat <- pmax(0, interp_habitat / max(interp_habitat))

habitat_frame$habitat <- interp_habitat

habitat_frame %>%
  ggplot(aes(rough_x, rough_y, fill = habitat)) +
  geom_tile()

  hab <- habitat_frame %>%
    dplyr::select(contains("rough"), habitat) %>%
    rename(x = rough_x, y = rough_y) %>%
    ungroup()

  out <- tibble(scientific_name = species, habitat = list(hab))


}


mats <- map(mats, safely(get_layer))

mats_worked <- map(mats, "error") %>% map_lgl(is.null)

mats <- mats[mats_worked]

mats <- map(mats, "result")

catch_mats <- mats %>%
  bind_rows() %>% 
  unnest(cols = habitat) %>% 
  group_by(scientific_name, x, y) %>% 
  summarise(habitat = sum(habitat)) %>% 
  ungroup() %>% 
  nest(habitat = c(-scientific_name)) %>% 
  ungroup()

catch_habitat <- catch_mats %>%
  bind_rows() %>%
  unnest(cols = habitat) %>%
  ggplot(aes(x, y, fill = habitat)) +
  geom_tile() +
  facet_wrap( ~ scientific_name) +
  scale_fill_viridis_c()

catch_habitat


```

Compare to the SDM stuff

```{r}

mats <- list.files(path = here("data", "matrices"))

mats <- mats[str_detect(mats, "longline_wcpfc")]

get_layer <- function(file) {
  # file <- mats[3]

  file_comps <- str_split(file, "_", simplify = TRUE)

  species <- str_replace(file_comps[, 1], "-", " ")

  fleet <- file_comps[, 2]


  hab <-
    read_csv(here("data", "matrices", file),
             col_names = FALSE,
             skip = 1) %>%
    rename(lat = X1) %>%
    mutate(lat = 1:nrow(.)) %>%
    pivot_longer(
      -lat,
      names_to = "lon",
      values_to = "hab",
      names_prefix = "X"
    ) %>%
    mutate(hab = tidyr::replace_na(hab, 0),
           lon = as.numeric(lon) - 1)

  # hab %>%
  #   ggplot(aes(lon, lat, fill = hab)) +
  #   geom_tile()
  #

  x_binsize <- (max(hab$lon) - min(hab$lon) + 1) / resolution

  y_binsize <- (max(hab$lat) - min(hab$lat) + 1) / resolution

  if (x_binsize < 1 | y_binsize < 1) {
    stop("resolution is too high for input data")
  }


  hab <- hab %>%
    mutate(X = lon - min(lon),
           Y = lat - min(lat)) %>%
    mutate(rough_x = floor(X / x_binsize),
           rough_y = floor(Y / y_binsize)) %>%
    select(contains("rough_"), hab)
  #
  #   hab %>%
  #     ggplot(aes(rough_x, rough_y, fill = hab)) +
  #     geom_tile()

  hab <- hab %>%
    group_by(rough_x, rough_y) %>%
    summarise(hab = sum(hab)) %>%
    mutate(xy = rough_x * rough_y)

  mod <-
    gamm4::gamm4(hab ~ s(rough_x) + s(rough_y) + s(xy), data = hab)

  hab$interp_cpue <- as.numeric(predict(mod$gam))

  hab$habitat <- pmax(0, hab$interp_cpue / max(hab$interp_cpue))


  if (sqrt(nrow(hab)) != resolution) {
    stop("habitat layer does not match simulation resolution")
  }
  hab <- hab %>%
    select(contains("rough"), habitat) %>%
    rename(x = rough_x, y = rough_y) %>%
    ungroup()

  # hab %>%
  #   ggplot(aes(rough_x, rough_y, fill = habitat)) +
  #   geom_tile()

  out <- tibble(scientific_name = species, habitat = list(hab))


}


mats <- map(mats, get_layer)

sdm_habitat <- mats %>%
  bind_rows() %>%
  unnest(cols = habitat) %>%
  filter(scientific_name %in% unique(catch_mats$scientific_name)) %>% 
  ggplot(aes(x, y, fill = habitat)) +
  geom_tile() +
  facet_wrap( ~ scientific_name) +
  scale_fill_viridis_c()


sdm_habitat


catch_habitat / sdm_habitat
```


# RPUE vs REvenue vs PPUE

So there are clearly some differences between the outcomes with the three fleet models. 

when the fleet distributes itself based on RPUE, as you start to fish things down the "best" patches start to look worse relative to the worst patches. This is because as you start to allocate more and more effort towards the high revenye patches, ther marginal benefit goes down, since the marginal increase in f per unit increase in E starts to decrease. So, the best revenye patch is 53 times better than the worst, but the best RPUE patch is only 7 time bigger than the worst. 

So RPUE is closer to PPUE, since it captures the effect that the demonitator starts to grow faster than the numerator at high E. Whereas with just revenue, as you increase effort the value of that patch just keeps going up, assuming a constant supply of fish. So you're including the "stock effect" sort of, and then with PPUE you add in the fact that not only do you get diminishing returns in E, but it costs you money to exert E. 

The revenue model says that even if it takes you infinite effort to get 101 units, and 1 effort to get 100 units, go to 101 units. 

So that bets two questions. 1 is there any logical reason for revenue in a real world setting where it would be distributed by revenue, not by RPUE? 

A fleet's objective function is maximize profits, either individually or in total. Let's think about in total then. The fleet will pick the the total amount of effort that maximizes profits, conditional on distributing those profits to mazimize profits per unit effort, right? 

The other is, did the solving the increased revenus through overfishing by mPA get solved by the open access or by the fleet model? GOod test, set alpha = 0, set fleet to RPUE, but also open access, see if that fixes it. 

If it doesn't another thing to consider it to modify the objective function to maximize RPUE or something like that to try and reflect costs 

the fundamental issues is that the fleet model and the optimization can have two different objectives. When you allocate around RPUE and optimize around revenue, the fleet is "suboptimal" since it allocqtes a bit less effort to the best patches, since their RPUE is a little lower thanks to the baranov. 

When you allocate by revenue you are least being internally consistent, i.e. the fleet is just maximizing revenue, and then you optimize around that. So, that seems like the best option until you figure out how to deal with profits / costs intelligently. 

Which is a strategy: shift the optimization to be around profits not revenue, which because you are assuming constant effort unless you include an exponent on effort, which causes a whole new set of problems since minute differences in the spatial distribution can change the outcomes, then optimizing around RPUE should be the same as optimizing around profits? So changing the optimization to profits should do nothing. becuase it's just going to be the same thing, except minus cost * constant effort. 

The only reason anyof this really matters is it if has a big impact on the bycatch implications. So, go back and run the experiments. If the revenue vs rpue this is intrinsic, then you need to consider it. If it's just a trait ot he WCPO scenario, then interestinh and flag and use this to discuss the critical need for better fleet modeling with Jim. 

Fishing the line is the culprit i think. in order for unintended consequences to occue, species have to live in different places. when you disteirbute by revenue, fishing the line is much more pronouned: people stick by the MPA edge. But, when you do RPUE, the near-MPA area starts to lose its appeal after a bit and people wander to new areas, causing problems. 

So, I think the solution is

1. store profit per fleet per patch in simmar so you can stop having to wrangle it

2. optimize around profits

3. Distribute fleet based on profits 

4. Turn off open access

This means that if you assume 0 profits at start, then you can only go down profit wise from overfished, but could in theory go up from rebuilding. 

So, that means that you're capturing the assumption that there's a reason why things are underfished now, but still allows for rebulding benefits

In order for this to have any spatial effect you need to have an exponent on effort cost or something like that. Otherwise, maximizing revenue is the exact same thing as maximizing profits - a constant (base_effort * cost_per_unit_effort). 

All of this serves two purposes. 1st to make things internally consistent for the optimization and the fleet model. 2nd to stop the "optimal" thing being to corral the fleet into catching more. \

An alternative would be to introduce a port and include a distance from fleet thing? But I think you can rationalize the exponent based on competition, gear competition (e.g. need to pull in longline and reset a bunch), and hold capacity? though does that really hold in the real world? I think it would have to be things like longer soak times needed?


The other thing that matter here is how you are thinking about the market. If it's one mobile fleet  then it makes sense ot pool revenues and costs everywhere. But, if it were a bunch of independent fleets in each patch, that get some displaced into them, then the decreased profit margins in indiivual patches start to matter

OK the only thing clear in this is that you do nede to actually run the damn counterfactual, it doesn't take long soj ust do it. 

sigh ok calling it. But for tomorrow, the key is to go back into simmar, and calculate profits by species, by first tallying fleet wide profits, and then dividing by the proportino of revenue by species 

Within the simulation model, if the spatial allocation was RPUE, then effort in the current time step would be exactly proportional to RPUE in the prior time step. 

You don't see that in the data. You do see a strong correlation with revenue in the prior time step. Really hard to say if that is causal, i.e. are fleets chasing revenue or is it a site effect or what, but it's at elast a piece of evidence. So, I think you go with revenue in the model, and use the experiments to make the case in the discussion the discussion that any tactical modeling needs to understand how the fleet actually responds and distributes itself. 

Choice depends a lot on the unts in question. IF you think that every unit of effort is discrete and per vessel, i.e. vessel I does or does not fish in that patch, then RPUE woul be the metric

# 	movement stuff

Very rough height of WCPO is 4800

At a resolutino of 10, that's


480 NM per cell 

Median tag distance was  95% within 3614 M.


4000 / 480

Schaefer 2015 is the paper 

design matter most in the in between sizes
